{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA663 Final Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the Bayesian hierarchical clustering algorithm described in the paper by Heller and Ghahramani (2005) and optimized the algorithm's speed using Cython. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is an unsupervised learning algorithm that organizes data into a binary tree. By cutting the tree at various heights, one can acquire clustering structures of the data. The traditional hierarchical clustering algorithm is the  agglomerative hierarchical clustering method. This algorithm starts with each data point being in it's own cluster and uses a bottom-up approach of iteratively merging the most similar clusters until all the data has been grouped into one cluster. The similarity of clusters is evaluated using a pre-specified distance measure (e.g. Euclidean distance).\n",
    "\n",
    "The major drawback of the traditional hierarchical clustering method is that it does not provide intuitive guidance to choosing the correct number of clusters or the appropriate distance measure. Furthermore, the algorithm does not define a probabilistic model for the data, thus it is not able to give predictions or cluster new data points into existing clusters.\n",
    "\n",
    "Heller and Ghahramani (2005) proposed the Bayesian hierarchical clustering model to overcome those aforementioned problems. The model is similar to the traditional hierarchical clustering method, but Bayesian hierarchical clustering merges data using the marginal likelihood of data. The major advantage of Bayesian hierarchical clustering over the traditional method is that it has a natural way to choose the correct number of clusters, and it can also give predictions of the probability of assigning new data into existing clusters. On the other hand, the disadvantage of the algorithm is that it is more computationally intensive since it requires calculating the marginal likelihood at each step. Bayesian hierarchical clustering works best when the user has no prior knowledge on the number of clusters or when the user wants to predict the probability of new data belonging to any existing clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Bayesian hierarchical clustering algorithm starts with every data point being in its own node. Afterwards, it iteratively merge the nodes with the highest merge probability until there is only a single node left (all data points in the same node). Merge probability is defined as the the ratio of the marginal likelihood of the two nodes being in the same cluster against the marginal likelihood of the nodes being partitioned in all possible ways which does not violate the existing tree structure. The tree can then be cut at the step where the merge probability is less than 50%.\n",
    "\n",
    "In mathematical terms, if we represent the hypothesis that the nodes are in the same cluster by $H_1^k$ and the tree structure using $T$, then we can write our the algorithm:\n",
    "\n",
    "1.Input: data = {$x^{(1)}, ..., x^{(n)}$}, model(likelihood) $p(x \\mid \\theta)$, prior $p(\\theta \\mid \\beta)$\n",
    "\n",
    "2.Initialize: number of clusters $c = n$, $D_i = $ {$x^{(i)}$} for $i = 1, ..., n$\n",
    "\n",
    "3.while $c > 1$ do\n",
    "\n",
    "Find the pair of clusters $D_i$ and $D_j$ that has the highest merge probability: $r_k = \\frac{\\pi_k p(D_k \\mid H_1^k)}{p(D_k \\mid T_k)}$ \n",
    "        \n",
    "Merge $D_i, D_j$ into $D_k$, $T_i$, and $T_j$ into $T_k$\n",
    "\n",
    "Delete $D_i, D_j$ and $c = c - 1$\n",
    "\n",
    "end while\n",
    "\n",
    "To calculate the merge probabilities, we need to compute $p(D_k \\mid T_k)$, which is\n",
    "$$\n",
    "p(D_k \\mid T_k) = \\pi_k p(D_k \\mid H_1^k) + (1 - \\pi_k) p(D_i \\mid T_i)p(D_j \\mid T_j)\n",
    "$$\n",
    "The term $\\pi_k$ can be calculated by\n",
    "\n",
    "1.Initialize: each leaf $i$ with $d_i = \\alpha, \\pi_i = 1$ \n",
    "\n",
    "2.for each internal node $k$ do\n",
    "\n",
    "$d_k = \\alpha \\Gamma(n_k) + d_{left_k}d_{right_k}$\n",
    "\n",
    "$\\pi_k = \\frac{\\alpha \\Gamma(n_k)}{d_k}$\n",
    "\n",
    "end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications to simulated data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def purity_score(cluster_true, cluster_pred):\n",
    "    \"\"\"\n",
    "    Function that returns the purity scores of each cluster given the predicted clusters and the true clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    def most_common(lst):\n",
    "        \"\"\"Helper function that finds the most frquent item in a list\"\"\"\n",
    "        return max(set(lst), key = lst.count)\n",
    "    \n",
    "    purity = []\n",
    "    for cluster in set(clust_pred):\n",
    "        np.where(np.array(cluster_pred) == cluster)\n",
    "        cluster_element = [cluster_true[index] for index in np.where(np.array(cluster_pred) == cluster)[0]]\n",
    "        dominant_item = most_common(cluster_element)\n",
    "        purity.append(cluster_element.count(dominant_item) / len(cluster_element))\n",
    "        \n",
    "    return purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications to real data sets\n",
    "We tested our BHC algorithm on the glass dataset mentioned in the original paper, as well as the famous iris dataset by Fisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glass = pd.read_csv(\"data/glass.data\", index_col=0, header=None)\n",
    "iris = pd.read_csv(\"data/iris.data\", header=None)\n",
    "glass_x = glass.iloc[:, 0:9]\n",
    "iris_x = iris.iloc[:, 0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative analysis with competing algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heller, K. A., & Ghahramani, Z. (2005). Bayesian Hierarchical Clustering. Neuroscience, 6(section 2), 297â€“304. doi:10.1145/1102351.1102389\n",
    "\n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
